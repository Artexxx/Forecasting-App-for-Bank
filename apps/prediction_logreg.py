import joblib
from keras.callbacks import EarlyStopping
from keras.layers import Input, Dense, Dropout, BatchNormalization
from keras.metrics import AUC
from keras.models import Model
from keras.optimizers import Adam
from keras_tuner import RandomSearch
from pathlib import Path
from plotly.io import templates
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from streamlit_option_menu import option_menu
import json
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go
import seaborn as sns
import streamlit as st
import warnings
sns.set_style("whitegrid")
templates.default = "plotly_white"
warnings.filterwarnings('ignore')


@st.cache_data
def create_correlation_matrix(df, features):
    corr = df.corr().round(2)
    fig1 = ff.create_annotated_heatmap(
        z=corr.values,
        x=list(corr.columns),
        y=list(corr.index),
        colorscale='ice',
        annotation_text=corr.values
    )
    fig1.update_layout(height=1000)

    # Выбираем только корреляцию с целевым признаком 'cnt'
    corr = corr['TARGET'].drop('TARGET')
    corr = corr[abs(corr).argsort()]
    fig2 = go.Figure()
    fig2.add_trace(go.Bar(
        x=corr.values,
        y=corr.index,
        orientation='h',
        marker_color=list(range(len(corr.index)))
    ))
    fig2.update_layout(
        title='Корреляция с TARGET',
        height=700,
        xaxis=dict(title='Признак'),  # Название оси x
        yaxis=dict(title='Корреляция'),
    )
    return fig1, fig2


@st.cache_data
def create_plot_roc_curve(y_true, y_prob):
    fpr, tpr, thresholds = roc_curve(y_true, y_prob)

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=fpr, y=tpr,
        mode='lines',
        name='ROC кривая (AUC = %0.2f)' % roc_auc_score(y_true, y_prob)
    ))
    fig.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1],
        mode='lines',
        line=dict(dash='dash'),
        name='Диагональ'
    ))

    fig.update_layout(
        xaxis_title='Доля ложно-положительных результатов',
        yaxis_title='Доля истинно-положительных результатов',
        title='Кривая ROC',
        width=900
    )
    return fig


@st.cache_data
def create_plot_confusion_matrix(y_true, y_pred, normalize=False):
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = cm.round(3)

    fig = px.imshow(
        cm,
        labels=dict(x="Предсказанный класс", y="Истинный класс"),
        x=['Нет', 'Да'], y=['Нет', 'Да'],
        title='Нормализованная матрица ошибок' if normalize else 'Матрица ошибок',
        color_continuous_scale='Blues'
    )

    fig.update_xaxes(tickangle=45, tickmode='array', tickvals=np.arange(2), ticktext=['Нет', 'Да'])
    fig.update_yaxes(tickangle=45, tickmode='array', tickvals=np.arange(2), ticktext=['Нет', 'Да'])
    # Добавление надписей
    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            fig.add_annotation(
                x=i, y=j,
                text=str(cm[j, i]),
                showarrow=False,
                font=dict(color="white" if cm[j, i] > thresh else "black"),
                align="center"
            )
    return fig


def score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        prob = clf.predict_proba(X_train)[:, 1]
        clf_report = classification_report(y_train, pred, output_dict=True)
        st.subheader("Результат обучения:")
        st.write(f"Точность модели: {accuracy_score(y_train, pred) * 100:.2f}%")
        st.write("ОТЧЕТ ПО КЛАССИФИКАЦИИ:", pd.DataFrame(clf_report).transpose())
        c1, c2 = st.columns(2)
        c1.plotly_chart(create_plot_roc_curve(y_train, prob), use_container_width=True)
        c2.plotly_chart(create_plot_confusion_matrix(y_train, pred, normalize=True), use_container_width=True)
    else:
        pred = clf.predict(X_test)
        prob = clf.predict_proba(X_test)[:, 1]
        clf_report = classification_report(y_test, pred, output_dict=True)
        st.subheader("Результат тестирования:")
        st.write(f"Точность модели: {accuracy_score(y_test, pred) * 100:.2f}%")
        st.write("ОТЧЕТ ПО КЛАССИФИКАЦИИ:", pd.DataFrame(clf_report).transpose())
        c1, c2 = st.columns(2)
        c1.plotly_chart(create_plot_roc_curve(y_test, prob), use_container_width=True)
        c2.plotly_chart(create_plot_confusion_matrix(y_test, pred, normalize=True), use_container_width=True)


def print_model_adequacy_section(current_dir: Path):
    st.markdown(
        """
        ## Оценка адекватности модели
        При оценки адекватности модели важно использовать несколько метрик, которые помогают оценить различные аспекты производительности модели.
 
        ### Матрица ошибок (Confusion Matrix)
 
        Матрица ошибок позволяет визуально оценить, как модель справляется с каждым из классов задачи. Она показывает, сколько примеров, предсказанных в каждом классе, действительно принадлежат этому классу.
        """
    )
    st.image(str(current_dir / 'images' / 'matrix.jpg'))
    st.markdown("""
        ### Отчет о классификации (Precision, Recall, F1-Score)
        * Precision (Точность) описывает, какая доля положительных идентификаций была верной (TP / (TP + FP)).
        * Recall (Полнота) показывает, какая доля фактических положительных классов была идентифицирована (TP / (TP + FN)).
        * F1-Score является гармоническим средним Precision и Recall и помогает учесть обе эти метрики в одной.

        ### Кривая ROC и площадь под кривой AUC
        * ROC кривая (Receiver Operating Characteristic curve) помогает визуально оценить качество классификатора. Ось X показывает долю ложноположительных результатов (False Positive Rate), а ось Y — долю истинноположительных результатов (True Positive Rate).
        * AUC (Area Under Curve) — площадь под ROC кривой, которая дает количественную оценку производительности модели.
    """)


@st.cache_data
def Information_Value(x, y):
    # Функция расчёта Information Value
    df = pd.DataFrame({'x': x, 'y': y})
    good = df.groupby('x')['y'].sum() / np.sum(df['y'])
    bad = (df.groupby('x')['y'].count() - df.groupby('x')['y'].sum()) / (len(df['y']) - np.sum(df['y']))
    WOE = np.log((good + 0.000001) / bad)
    IV = (good - bad) * WOE
    return IV.sum()


@st.cache_data
def compute_iv_for_features(df, target_column):
    iv_values = {
        feature: Information_Value(df[feature],
        df[target_column])
            for feature in df.columns
            if feature != target_column
    }
    return iv_values


@st.cache_data
def encode_features(df, features):
    encoders = {}
    for feature in features:
        le = LabelEncoder()
        df[feature] = le.fit_transform(df[feature])
        encoders[feature] = le
    return df, encoders


def app(df, current_dir: Path):
    st.title("Прогнозирование просрочек у клиентов банка")
    st.image(str(current_dir / "images" / "main2.webp"), width=150, use_column_width='auto')

    st.markdown(
        """
        # Подготовка набора данных
        На этом этапе мы готовим данные для дальнейшего анализа и прогнозирования. Подготовка данных включает обработку пропусков, преобразование категориальных переменных и масштабирование числовых признаков.
        """
    )
    df_encoded = df.copy(deep=True)
    st.title("Преобразование категориальных переменных")

    st.markdown("""
        ### Преобразование категориальных переменных

        Сделаем кодирование категориальных переменных с помощью LabelEncoder из библиотеки Scikit-Learn. Это преобразует категории в числа, что необходимо для использования этих данных в большинстве алгоритмов машинного обучения.

        #### Label Encoding

        Формула кодирования конкретной категории $C$ в число может быть представлена как: $ \\text{code}(C) = i $
        где $i$ — порядковый номер категории $C$ в данных.

        #### Общие формулы и принципы

        В процессе предобработки данных могут использоваться следующие операции и принципы:

        - **Нормализация:** приведение всех числовых переменных к единому масштабу, чтобы улучшить сходимость алгоритма. Обычно используется Min-Max scaling или Z-score стандартизация.
        - **One-hot Encoding:** преобразование категориальных переменных в бинарные векторы; применяется, когда порядок категорий не имеет значения.
        - **Отбор признаков:** удаление нерелевантных или малоинформативных признаков для упрощения модели и улучшения её обобщающей способности.

        Эти методы помогают подготовить данные для эффективного обучения моделей машинного обучения.
        """)
    categorical_features = df.select_dtypes(include=['category', 'object']).columns.tolist()
    numerical_features = df.select_dtypes(include=['int', 'int32', 'int64', 'float64']).columns.tolist()

    df_raw = df.copy(deep=True)
    df, encoders = encode_features(df, categorical_features)
    st.markdown(
        """
        ## Кодирование признаков (Feature Encoding)
        """
    )
    feature_encoding_tab1, feature_encoding_tab2 = st.tabs([
        "Данные до Feature Encoding",
        "Данные после Feature Encoding"
    ])
    with feature_encoding_tab1:
        st.dataframe(df_raw.head(15))
    with feature_encoding_tab2:
        st.dataframe(df.head(15))

    # st.markdown(r"""
    #     ## Введение в WOE (Weight of Evidence)
    #     WOE (Weight of Evidence) обеспечивает полезный способ обработки и преобразования категориальных переменных в числовые, что является особенно важным для алгоритмов машинного обучения, таких как логистическая регрессия, которые требуют числового ввода. WOE также позволяет интерпретировать влияние предикторов после преобразования. Перед тем как применять WOE, часто целесообразно разбить числовые признаки на интервалы. Процесс разбиения может быть автоматизирован с помощью методов, таких как деревья решений, которые помогают определить оптимальные точки разбиения.
    #     ### Решение
    #     WOE можно считать для числовых признаков, однако, учитывая формулу WOE, делать это для числовых признаков не очень правильно. Лучше разбить диапазон значений на интервалы, закодировать их и посчитать WOE. Отсюда вытекает задача, как оптимально разбить значения признаков на интервалы. Для решения этой задачи, мы решили для каждого признака обучать DecisionTreeClassifier и из него получать границы разбиение признака, при которых score максимальный.
    #
    #     ### Получение границ разбиения
    #     Рассчитаем границы разбиений для каждого признака. Посмотрим параметры дерева, которое делит признаки на интервалы, а также на само разбиение.
    #
    #     ### Зачем мы разделили диапазон значений на интервалы
    #
    #     #### Дискретизация признаков
    #     Одной из целей использования границ разбиения является **дискретизация непрерывных переменных**. Дискретизация (или биннинг) преобразует непрерывные переменные в категориальные, что может помочь улучшить производительность некоторых алгоритмов классификации, уменьшить влияние выбросов и упростить модель, делая ее интерпретируемой.
    #
    #     #### Улучшение интерпретируемости
    #     Категории, полученные в результате дискретизации, позволяют легче интерпретировать влияние признаков на модель, поскольку каждый интервал можно анализировать отдельно в контексте целевой переменной.
    #
    #     #### Кодирование Weight of Evidence (WOE)
    #     После дискретизации признаков значения, присвоенные каждому интервалу, используются для расчета **Weight of Evidence (WOE)**. WOE является мощной техникой в кредитном скоринге и риск-менеджменте, позволяющей оценить влияние предиктора на вероятность наступления события (например, дефолта по кредиту).
    #
    #     Формула для расчета WOE для каждой группы:
    #     $$ \text{WOE} = \ln \left(\frac{\text{Распределение хороших клиентов}}{\text{Распределение плохих клиентов}}\right) $$
    #     где "хорошие" и "плохие" клиенты - это классы в вашей целевой переменной `y`.
    #
    #     #### Пример использования
    #
    #     В вашем контексте, дискретизация и последующий расчет WOE могут помочь в следующем:
    #     - **Улучшение производительности моделей**: Некоторые алгоритмы, особенно логистическая регрессия, работают лучше, если предикторы кодируются через WOE.
    #     - **Управление риском**: Расчет WOE позволяет более точно оценить вклад каждого признака в риск, что критически важно в финансовой сфере.
    #     - **Повышение интерпретируемости**: Интервалы и соответствующие им значения WOE можно легко интерпретировать и объяснить в бизнес-терминах, что делает модель более прозрачной.
    #
    #     #### Использование WOE
    #
    #     1. Разделение Переменных: Прежде чем можно вычислить WOE, необходимо разделить каждый признак на группы (используя функцию splitter и границы, найденные ранее). Это позволяет сравнивать разные уровни риска в разных группах переменных.
    #     2. Преобразование в WOE: После разделения переменных значения в каждой группе преобразуются в WOE, что помогает в оценке влияния каждой группы на вероятность исхода.
    #     3. Интерпретация: Больший WOE указывает на больший риск (или меньшую вероятность хорошего исхода), а меньший WOE — на меньший риск
    #
    # """)
    # Функция, которая возвращает границы разбиений
    # @st.cache_data
    # def get_bondaries(x_bondaries, y_bondaries):
    #     parameters = {
    #         'max_depth': [x for x in range(1, 21)],
    #         'min_samples_leaf': [5, 10, 20, 30, 50, 70, 100, 150, 200, 300, 400, 500, 1000, 2000, 5000, 10000, 20000]
    #     }
    #     dtc = DecisionTreeClassifier(random_state=17)
    #     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)
    #     clf = GridSearchCV(dtc, parameters, scoring='roc_auc', cv=skf)
    #     clf.fit(pd.DataFrame(x_bondaries), y_bondaries)
    #     print('Best parameters for DT: ', clf.best_params_)
    #     print('ROC_AUC score: ', round(clf.best_score_, 4))
    #     tree = clf.best_estimator_
    #     tree.fit(pd.DataFrame(x_bondaries), y_bondaries)
    #     print('Boundaries: ', np.sort([x for x in tree.tree_.threshold if x != -2]))
    #     return np.sort([x for x in tree.tree_.threshold if x != -2])
    #
    # Функция, которая режет признак на интервалы и кодирует их
    # def splitter(x, col_bondaries):
    #     for i in range(len(col_bondaries)):
    #         if i > 0:
    #             if x > col_bondaries[i - 1] and x <= col_bondaries[i]:
    #                 return i
    #         if i == 0:
    #             if x <= col_bondaries[i]:
    #                 return i
    #         if i == len(col_bondaries) - 1:
    #             if x > col_bondaries[i]:
    #                 return i + 1
    #
    # # Функция расчёта WOE
    # def WOE(x, y):
    #     df = pd.DataFrame({'x': x, 'y': y})
    #     good = df.groupby('x')['y'].sum() / np.sum(df['y'])
    #     bad = (df.groupby('x')['y'].count() - df.groupby('x')['y'].sum()) / (len(df['y']) - np.sum(df['y']))
    #     WOE = np.log((good + 0.000001) / bad)
    #     WOE = pd.Series(WOE).to_dict()
    #     return x.apply(lambda x: WOE.get(x))
    #
    #
    # # bondaries = dict()
    # # for col in df.columns:
    # #     if col != 'TARGET':
    # #         print(col)
    # #         bondaries[col] = get_bondaries(df[col], df["TARGET"])
    # #         print('--------------')
    # from numpy import array
    #
    #
    # @st.cache_data()
    # def pain_in_ass(df):
    #     bondaries ={'AGE': array([25.5, 30.5, 38.5, 40.5, 42.5, 45.5, 49.5, 52.5, 56.5, 61.5]),
    #      'REGION': array([3.5, 10.5, 12.5, 25.5, 29.5, 30.5, 37.5, 39.5, 42.5, 44.5, 47.5,
    #                       48.5, 51.5, 64.5, 66.5, 69.5, 73.5, 75.5]),
    #      'GENDER': array([1.5]),
    #      'INCOME': array([9700., 11500., 12500., 14500., 15063., 16600., 17250.,
    #                       19400., 20450., 21425., 22900., 23950., 24100., 25450.,
    #                       26750., 27500., 28950., 29500., 30700., 31925., 32250.,
    #                       34750., 35250., 36750., 37250., 38250., 39550., 40250.,
    #                       41750., 42800., 43250., 44500., 45250., 46970., 47250.,
    #                       48350., 49950., 50100., 50950., 51500., 52819., 53300.,
    #                       54750., 55350., 56250., 57800., 59300., 60400., 61400.,
    #                       62900., 63250., 64600., 65500., 66850., 67500., 68035.,
    #                       69050., 70500., 71500., 72150., 73750., 74500., 75100.,
    #                       77500., 79500., 81875., 82400., 84250., 85500., 89500.,
    #                       90500., 94500., 95500., 97500., 99900., 103550., 108500.,
    #                       112490., 114950., 115500., 119500., 122500., 124500., 126500.,
    #                       132504., 137500., 144000., 146000., 150125., 158000., 162500.,
    #                       194000., 198500., 199500., 202500., 248500., 255000., 337500.,
    #                       355000., 405000., 490800.]),
    #      'MARITAL_STATUS': array([0.5, 3.5, 4.5]),
    #      'IP_FLAG': array([], dtype='float64'),
    #      'SME_FLAG': array([], dtype='float64'),
    #      'EMPLOYEE_FLAG': array([], dtype='float64'),
    #      'REFUGEE_FLAG': array([], dtype='float64'),
    #      'PDN': array([3.45000005, 4.04999995, 4.75, 5.35000014,
    #                    6.25, 6.54999995, 7.04999995, 7.35000014,
    #                    7.75, 8.25, 8.8499999, 9.55000019,
    #                    9.94999981, 10.3499999, 10.8499999, 11.1500001,
    #                    11.6500001, 12.75, 13.25, 13.94999981,
    #                    14.75, 14.94999981, 15.25, 16.34999943,
    #                    20.25, 23.44999981, 23.84999943, 24.34999943,
    #                    24.94999981, 25.75, 26.15000057, 26.65000057,
    #                    26.84999943, 27.25, 27.94999981, 28.55000019,
    #                    28.75, 30.15000057, 30.34999943, 30.75,
    #                    31.25, 31.65000057, 32.25, 32.54999924,
    #                    33.04999924, 33.45000076, 33.75, 34.14999962,
    #                    34.35000038, 34.75, 35.04999924, 35.39999962,
    #                    35.64999962, 36.04999924, 36.25, 36.64999962,
    #                    37.25, 37.54999924, 38.04999924, 38.64999962,
    #                    39.04999924, 39.35000038, 39.75, 40.14999962,
    #                    40.35000038, 40.54999924, 40.85000038, 41.54999924,
    #                    41.85000038, 42.54999924, 42.85000038, 43.25,
    #                    43.54999924, 43.85000038, 44.04999924, 44.35000038,
    #                    44.54999924, 44.75, 45.04999924, 45.45000076,
    #                    45.64999962, 46.25, 46.45000076, 47.04999924,
    #                    47.45000076, 47.80000114, 48.04999924, 48.35000038,
    #                    48.54999924, 48.75, 48.95000076, 49.04999924,
    #                    49.35000038, 49.54999924, 50.04999924, 50.45000076,
    #                    50.64999962, 50.95000076, 51.14999962, 51.45000076,
    #                    51.85000038, 52.14999962, 52.54999924, 52.95000076,
    #                    53.25, 53.54999924, 53.75, 54.04999924,
    #                    54.25, 54.45000076, 55.04999924, 55.35000038,
    #                    55.75, 56.14999962, 56.35000038, 56.64999962,
    #                    57., 57.35000038, 57.95000076, 58.45000076,
    #                    58.95000076, 59.35000038, 59.75, 62.14999962,
    #                    62.64999962, 63.04999924, 63.45000076, 64.14999771,
    #                    64.54999924, 65.25, 65.75, 66.04999924,
    #                    66.95000076, 67.25, 67.54999924, 68.14999771,
    #                    69.04999924, 69.45000076, 70.14999771, 70.95000076,
    #                    71.64999771, 72., 73.14999771, 75.04999924,
    #                    75.95000076, 76.25, 77.64999771, 78.14999771,
    #                    79.14999771, 80.45000076, 80.70000076, 81.54999924,
    #                    82.09999847, 83.35000229, 84.14999771, 84.75,
    #                    86.14999771, 87.75, 88.14999771, 91.,
    #                    92.14999771, 93.25, 93.64999771, 94.64999771,
    #                    99.04999924, 100.35000229, 102.14999771, 103.45000076,
    #                    105.25, 106.34999847, 112.20000076, 114.14999771,
    #                    116.75999832, 118.5, 125.85000229, 127.75,
    #                    130.5, 135.45000458, 138.5, 141.29999542,
    #                    150.69999695, 153.79999542, 157.5, 166.69999695,
    #                    180.44999695, 211.09999847, 261.8500061]),
    #      'CREDIT_TYPE': array([502., 603.5, 704.5, 706., 707.5, 708.5]),
    #      'CREDIT_PURCHASE': array([0.5, 1.5, 3.5, 10.5, 11.5, 12.5, 13.5, 15.5]),
    #      'PRODUCT_CODE': array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5]),
    #      'TERM': array([11., 14., 19., 26., 37.5, 43.5, 49.5, 58., 59.5]),
    #      'ORIG_AMOUNT': array([82061., 163952.4609375, 300708.859375, 515761.40625]),
    #      'CURR_RATE_NVAL': array([1.35000002, 2.5, 3.70000005, 4.04999995, 5.29999995,
    #                               6.25, 7.27499986, 7.95000005, 8.26000023, 8.43499994,
    #                               9.05000019, 9.94999981, 10.26499987, 10.36499977, 10.51499987,
    #                               10.88499975, 11.4749999, 12.54500008, 12.59500027, 13.3499999,
    #                               13.55000019, 13.65499973, 13.94999981, 14.1500001, 14.3499999,
    #                               14.44999981, 14.55000019, 14.6500001, 14.75, 14.96000004,
    #                               15.05000019, 15.1500001, 15.21999979, 15.41499996, 15.55000019,
    #                               15.625, 15.67000008, 15.69000006, 15.75, 15.82500029,
    #                               15.875, 15.94999981, 16.11999989, 16.32499981, 16.48499966,
    #                               16.59000015, 16.65500069, 16.75500011, 16.82499981, 16.94999981,
    #                               17.01500034, 17.06500053, 17.15000057, 17.25, 17.34999943,
    #                               17.52499962, 17.65000057, 17.80500031, 18.01000023, 18.13500023,
    #                               18.25, 18.44999981, 18.65000057, 18.94999981, 19.39999962,
    #                               19.55000019, 19.80000019, 19.95499992, 20.05000019, 20.17500019,
    #                               20.55000019, 20.84999943, 20.98999977, 21.15000057, 21.27499962,
    #                               21.34999943, 21.44999981, 21.65000057, 21.75, 21.84999943,
    #                               22.05000019, 22.43999958, 22.52499962, 22.63500023, 22.75,
    #                               22.94999981, 23.05000019, 23.28999996, 23.55000019, 23.80000019,
    #                               23.94999981, 24.60000038, 24.88999939, 27.75, 28.91499996,
    #                               32.14999962]),
    #      'OPEN_YEAR': array([2018.5, 2019.5, 2020.5, 2021.5, 2022.5, 2023.5]),
    #      'OPEN_MONTH': array([1.5, 7.5, 9.5]),
    #      'OPEN_DAY': array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5,
    #                         13.5, 14.5, 15.5, 17.5, 22.5, 24.5, 26.5, 27.5, 28.5, 29.5, 30.5]),
    #      'OPEN_DAYOFWEEK': array([0.5, 1.5, 4.5, 5.5])
    #     }
    #     if st.expander(label='Показать границы', expanded=False):
    #         st.write(bondaries)
    #     df['TARGET'] = df['TARGET'].astype('int64')
    #     data_woe = pd.DataFrame()
    #     for col in df.columns:
    #         if col != 'TARGET' and col not in (
    #         'IP_FLAG', 'INCOME', 'SME_FLAG', 'EMPLOYEE_FLAG', 'REFUGEE_FLAG', 'CARD_TYPE',):
    #             data_woe[col] = df[col].apply(lambda x: splitter(x, bondaries[col]))
    #             data_woe[col] = WOE(data_woe[col], df['TARGET'])
    #         else:
    #             data_woe[col] = df[col]
    #     return data_woe
    #
    # data_woe = pain_in_ass(df)
    # st.write(data_woe)

    st.markdown(r"""
        ### Information Value (IV)

        **Information Value (IV)** - это метрика, используемая для оценки предиктивной способности или силы признака в контексте бинарной классификации. Эта метрика помогает определить, насколько хорошо переменная может различать две группы (например, дефолт/не дефолт в кредитном скоринге). IV рассчитывается на основе Weight of Evidence (WOE), которая была рассчитана на предыдущих шагах.

        #### Формула для расчёта IV:

        $$ IV = \sum_{i} (\text{Good}_i - \text{Bad}_i) \times \text{WOE}_i $$

        где:
        - $Good_i$ - доля "хороших" клиентов (например, тех, кто своевременно выплачивает кредит) в группе $i$,
        - $Bad_i$ - доля "плохих" клиентов в той же группе,
        - $WOE_i$ - Weight of Evidence для группы $i$.

        ### Применение и интерпретация IV

        - **Низкое значение IV** (меньше 0.1) указывает, что переменная имеет очень слабую предиктивную способность.
        - **Среднее значение IV** (0.1 до 0.3) говорит о средней предиктивной способности.
        - **Высокое значение IV** (больше 0.3) показывает, что переменная является сильным предиктором.

        Посмотрим на Information Value категориальных признаков:
     """)

    with st.expander("**Показать IV**", expanded=False):
        iv_values = compute_iv_for_features(df, 'TARGET')
        for feature, iv in iv_values.items():
            st.write(f'IV для {feature} = {min(iv, 3):.4f}')

    st.header("Корреляционный анализ")
    st.markdown("""
           Матрица корреляции позволяет определить связи между признаками. Значения в матрице колеблются от -1 до 1, где:

           - 1 означает положительную линейную корреляцию,
           - -1 означает отрицательную линейную корреляцию,
           - 0 означает отсутствие линейной корреляции.
       """)
    fig1, fig2 = create_correlation_matrix(df, numerical_features)
    st.plotly_chart(fig1, use_container_width=True)

    markdown_col1, markdown_col2 = st.columns(2)
    markdown_col1.markdown("""
        Корреляционная матрица представляет связь между различными числовыми параметрами.
        Связь различных параметров с целевой переменной `TARGET`:
        
        - **ORIG_AMOUNT**: -0.03 – почти нет связи.
        - **CREDIT_PURCHASE**: 0.11 – легкая положительная связь.
        - **VALUE_DT_DAYOFWEEK**, **OPEN_DT_DAYOFWEEK**: 0.15 – умеренная положительная связь.
        - **TERM**, **VALUE_DT_DAY**, **OPEN_DT_DAY**, **CREDIT_TYPE**, **OPEN_DT_MONTH**, **VALUE_DT_MONTH**: 0.16-0.18 – умеренная положительная связь.
        - **CС_LIMIT_NVAL**: -0.2 – легкая отрицательная связь.
        - **CURR_RATE**: -0.22 – легкая отрицательная связь.
        - **PRODUCT_CODE**: 0.23 – умеренная положительная связь.
        - **СС_GRACE_PERIOD**: -0.24 – легкая отрицательная связь.
        - **VALUE_DT_YEAR**, **OPEN_DT_YEAR**, **CARD_TYPE**: 0.26 – умеренная положительная связь.
        - **INCOME**: -0.26 – умеренная отрицательная связь.
        - **PDN**: -0.31 – умеренная отрицательная связь.
        - **REGION**: -0.43 – умеренная отрицательная связь.
        - **CURR_RATE_NVAL**: 0.46 – умеренная положительная связь.
        - **GENDER**: -0.57 – умеренная отрицательная связь.
        - **AGE**: -0.58 – умеренная отрицательная связь.
        - **MARITAL_STATUS**: -0.61 – умеренная отрицательная связь.
        - **REFUGEE_FLAG**, **EMPLOYEE_FLAG**, **SME_FLAG**, **IP_FLAG**: -0.68 – -0.69 – сильная отрицательная связь.

       """)
    markdown_col2.plotly_chart(fig2, use_container_width=True)

    # st.subheader('Разделение данных')
    # X = df_encoded.drop('TARGET', axis=1).copy(deep=True)
    # Y = df_encoded['TARGET']
    #
    # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    from sklearn.preprocessing import MinMaxScaler
    from imblearn.over_sampling import SMOTE
    smote = SMOTE()
    X = df.drop('TARGET', axis=1)
    y = df['TARGET']

    # Применение SMOTE к данным
    X_resampled, y_resampled = smote.fit_resample(X, y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.3,
        stratify=y_resampled,
        random_state=17
    )
    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    X_train = pd.DataFrame(X_train, columns=X.columns)
    X_test = pd.DataFrame(X_test, columns=X.columns)

    st.write("Размер тренировочных данных:", X_train.shape, y_train.shape)
    st.write("Размер тестовых данных:", X_test.shape, y_test.shape)

    tab1, tab2 = st.tabs(["Тренировочные данные", "Тестовые данные"])

    with tab1:
        st.subheader("Тренировочные данные")
        st.markdown("""
          **Описание:** Тренировочные данные используются для подгонки модели и оценки её параметров.
          Эти данные получены путем исключения из исходного датасета столбцов с целевой переменной 'y'.

          **Данные тренировочного набора (X_train)**.
          Обучающий набор данных содержит информацию о признаках, используемых для обучения модели.
          """)
        st.dataframe(X_train, use_container_width=True)
        st.markdown("""
          **Целевая переменная (y_train)**.
          Целевая переменная содержит значения цены, которые модель должна научиться прогнозировать.
          В качестве целевой переменной для тренировочного набора используются исключительно значения столбца 'y'.
          """)
        st.dataframe(pd.DataFrame(y_train).T)

    with tab2:
        st.subheader("Тестовые данные")
        st.markdown("""
          **Описание:** Тестовые данные используются для проверки точности модели на данных, которые не участвовали в тренировке.
          Это позволяет оценить, как модель будет работать с новыми, ранее не виденными данными.
          """)
        st.markdown("""
          **Данные тестового набора (X_test)**.
          Тестовый набор данных содержит информацию о признаках, используемых для оценки модели.
          """)
        st.dataframe(X_test, use_container_width=True)
        st.markdown("""
          **Целевая переменная (y_test)**.
          Целевая переменная представляет собой значения, которые модель пытается предсказать.
          """)
        st.dataframe(pd.DataFrame(y_test).T)

    st.markdown(
        """
        # Моделирование
        ## Работа с несбалансированными данными
        Обратите внимание, что у нас есть несбалансированный набор данных, в котором большинство наблюдений относятся к одному типу ('NO'). В нашем случае, например, примерно 84% наблюдений имеют метку 'No', а только 16% - 'Yes', что делает этот набор данных несбалансированным.
        
        Для работы с такими данными необходимо принять определенные меры, иначе производительность нашей модели может существенно пострадать. В этом разделе я рассмотрю два подхода к решению этой проблемы.
        
        ### Увеличение числа примеров меньшинства или уменьшение числа примеров большинства
        В несбалансированных наборах данных основная проблема заключается в том, что данные сильно искажены, т.е. количество наблюдений одного класса значительно превышает количество наблюдений другого. Поэтому в этом подходе мы либо увеличиваем количество наблюдений для класса-меньшинства (oversampling), либо уменьшаем количество наблюдений для класса-большинства (undersampling).
        
        Стоит отметить, что в нашем случае количество наблюдений и так довольно мало, поэтому более подходящим будет метод увеличения числа примеров.
        
        Ниже я использовал технику увеличения числа примеров, известную как SMOTE (Synthetic Minority Oversampling Technique), которая случайным образом создает некоторые "синтетические" инстансы для класса-меньшинства, чтобы данные по обоим классам стали более сбалансированными.
        
        Важно использовать SMOTE до шага кросс-валидации, чтобы избежать переобучения модели, как это бывает при выборе признаков.
        
        ###  Выбор правильной метрики оценки
        Еще один важный аспект при работе с несбалансированными классами - это выбор правильных оценочных метрик.
        
        Следует помнить, что точность (accuracy) не является хорошим выбором. Это связано с тем, что из-за искажения данных даже алгоритм, всегда предсказывающий класс-большинство, может показать высокую точность. Например, если у нас есть 20 наблюдений одного типа и 980 другого, классификатор, предсказывающий класс-большинство, также достигнет точности 98%, но это не будет полезной информацией.
        
        В таких случаях мы можем использовать другие метрики, такие как:
        
        - **Точность (Precision)** — (истинно положительные)/(истинно положительные + ложно положительные)
        - **Полнота (Recall)** — (истинно положительные)/(истинно положительные + ложно отрицательные)
        - **F1-Score** — гармоническое среднее точности и полноты
        - **AUC ROC** — ROC-кривая, график между чувствительностью (Recall) и (1-specificity) (Специфичность=Точность)
        - **Матрица ошибок** — отображение полной матрицы ошибок
        """
    )

    st.markdown(
        r"""
        ### Логистическая регрессия
        Логистическая регрессия — это статистический метод анализа, используемый для моделирования зависимости дихотомической переменной (целевой переменной с двумя возможными исходами) от одного или нескольких предикторов (независимых переменных). Основное отличие логистической регрессии от линейной заключается в том, что первая предсказывает вероятность наступления события, используя логистическую функцию, что делает её идеальной для классификационных задач.
        
        #### Математическая модель
        
        ##### Логистическая функция (Сигмоид)
        Основой логистической регрессии является логистическая функция, также известная как сигмоид. Она описывается следующей формулой:
        
        $$
        P(y=1|X) = \frac{1}{1 + e^{-z}} 
        $$
        
        где $ z = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n $ — линейная комбинация входных переменных $ X $ (независимых переменных) и коэффициентов модели $ \beta $ (включая свободный член $ \beta_0 $ и коэффициенты при переменных $ \beta_1, \beta_2, ..., \beta_n $).
        
        ##### Интерпретация коэффициентов
        Коэффициенты в логистической регрессии интерпретируются через шансы (odds) и логарифм шансов:
        
        - **Шансы**: Вероятность того, что событие произойдет, деленная на вероятность того, что событие не произойдет.
        
        $$ \text{odds} = \frac{P(y=1|X)}{1 - P(y=1|X)} = e^z $$
        
        - **Логарифм шансов**:
        
        $$\log(\text{odds}) = z = \beta_0 + \beta_1x_1 + ... + \beta_nx_n $$
        
        Каждый коэффициент $\beta_i $ показывает, как изменится логарифм шансов, если соответствующая переменная увеличится на одну единицу, при условии, что все остальные переменные остаются неизменными.
        
        #### Регуляризация
        
        Регуляризация используется для предотвращения переобучения путем добавления штрафа за слишком большие значения коэффициентов к функции потерь:
        
        - **L1-регуляризация (Lasso)**:
        
          Штрафует сумму абсолютных значений коэффициентов. Это может привести к обнулению некоторых коэффициентов, что делает модель разреженной и может помочь в отборе признаков.
        
        - **L2-регуляризация (Ridge)**:
        
          Штрафует сумму квадратов коэффициентов, что предотвращает их слишком большое увеличение и помогает уменьшить переобучение, но не делает модель разреженной.
        
        #### Оценка модели
        
        Для оценки качества модели логистической регрессии часто используют ROC AUC (Area Under the Receiver Operating Characteristics Curve). Эта метрика
        
         помогает оценить, насколько хорошо модель может различать два класса (например, положительный и отрицательный).
        
        - **ROC AUC**:
          - Чем ближе значение ROC AUC к 1, тем лучше модель различает два класса.
          - Значение 0.5 говорит о том, что модель работает не лучше случайного гадания.
        
        #### Параметры в Логистической Регрессии:
        
        - **Penalty**: Тип регуляризации, используемый в модели (обычно `l1` или `l2`). Регуляризация помогает предотвратить переобучение модели путем штрафа за слишком большие коэффициенты.
          - `l1` (Lasso регуляризация): Штрафует абсолютное значение коэффициентов и может обнулять некоторые из них, делая модель разреженной.
          - `l2` (Ridge регуляризация): Штрафует квадраты коэффициентов, предотвращает их быстрый рост.
        
        - **C**: Инверсия силы регуляризации (`C = 1/λ`). Меньшие значения `C` указывают на сильнее регуляризацию.
        
        - **Class_weight**: Веса классов. Этот параметр используется, если классы в данных имеют различное количество образцов или если один класс более важен другого. `None` означает, что все классы имеют одинаковый вес, `balanced` автоматически взвешивает классы в соответствии с частотой их встречаемости.
        
        #### GridSearchCV
        
        GridSearchCV — это метод, который позволяет систематически просматривать множество комбинаций параметров, выбирая те, которые максимизируют качество модели на основе заданной метрики (в вашем случае — `roc_auc`).
        
        #### Процесс:
        1. **Задание параметров для поиска**: Создается словарь параметров, которые должны быть проверены.
        2. **Настройка кросс-валидации**: Определение метода кросс-валидации (здесь StratifiedKFold), который сохраняет соотношение классов в каждом фолде.
        3. **Подбор модели**: LogisticRegression подается в GridSearchCV вместе с параметрами и методом валидации.
        4. **Обучение**: GridSearchCV автоматически обучает модели на каждой комбинации параметров и валидирует их по стратегии кросс-валидации.
        
        #### Результаты:
        - **Best parameters**: Наилучшие параметры (`{'C': 0.02, 'class_weight': None, 'penalty': 'l2'}`), найденные в результате поиска.
        - **ROC_AUC score**: Наивысшее значение ROC-AUC (0.7958), которое показывает эффективность модели в различении двух классов (`yes` и `no`). Это значение близко к 1, что указывает на высокую предсказательную способность модели.
        """
    )
    try:
        def test1():
            return joblib.load(str(current_dir / "models" / 'logistic_regression_model.joblib'))

        log_reg = test1()
    except Exception as e:
        log_reg = LogisticRegression(class_weight='balanced', C=0.09, penalty='l2')
        log_reg.fit(X_train, y_train)
        joblib.dump(log_reg, str(current_dir / "models" / 'logistic_regression_model.joblib'))

    print_model_adequacy_section(current_dir)
    tab1, tab2 = st.tabs(["Результаты модели на зависимых данных", "Результаты модели на независимых данных", ])
    with tab1:
        score(log_reg, X_train, y_train, X_test, y_test, train=True)
    with tab2:
        score(log_reg, X_train, y_train, X_test, y_test, train=False)

    # Получение коэффициентов и интерсепта
    coef = log_reg.coef_[0]
    intercept = log_reg.intercept_[0]

    # Вывод формулы в формате Markdown
    formula = f"z = {intercept:.3f}"
    variables = X_train.columns

    for var, c in zip(variables, coef):
        sign = "+" if c >= 0 else "-"
        formula += f" {sign} {abs(c):.3f} \\cdot {var.replace('_', r'\_')}"
    st.markdown(r"""
        ## Полученная логистическая функция
        $$
        P(y=1|X) = \frac{1}{1 + e^{-z}} 
        $$
    """)
    st.markdown(f"Где\n\n $$ {formula} $$")

    st.markdown(r"""
        # Моделирование нейронной сети с использованием Keras Tuner
        
        ## Архитектура модели
        
        Архитектура модели нейронной сети, используемой в данном примере, состоит из нескольких слоев, каждый из которых играет определенную роль в обработке данных и улучшении обучения модели. Вот подробное описание каждого компонента архитектуры:
        
        1. **Входной слой (Input Layer)**: Определяет форму входных данных, которые будут подаваться на вход сети.
        2. **Полносвязные слои (Dense Layers)**: Классические нейронные слои, где каждый нейрон соединен со всеми нейронами следующего слоя.
        3. **BatchNormalization**: Нормализует выходы предыдущего слоя, что стабилизирует и ускоряет обучение.
        4. **Dropout**: Регуляризационная техника, предотвращающая переобучение путем случайного отключения нейронов во время обучения.
        5. **Выходной слой (Output Layer)**: Использует сигмоидальную функцию активации для вывода вероятности принадлежности к одному из классов.
        
        ## Формулы и теоретические концепции
        
        1. **Входной слой (Input Layer)**
        
        Формирует начальную структуру модели, принимая входные данные с заданным количеством признаков:
        $$
        \text{inputs} = \text{Input}(shape=(X_train.shape[1],))
        $$
        
        2. **Полносвязный слой (Dense Layer)**
        
        Применяет линейное преобразование и функцию активации ReLU к входным данным:
        $$
        \text{Dense}(units, activation='relu')
        $$
        Линейное преобразование:
        $$
        z = Wx + b
        $$
        Функция активации ReLU:
        $$
        f(z) = \max(0, z)
        $$
        
        3. **BatchNormalization**
        
        Нормализует выходы предыдущего слоя по следующим формулам:
        $$
        \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
        $$
        где $\mu$ и $\sigma^2$ – среднее и дисперсия батча, а $\epsilon$ – малое число для предотвращения деления на ноль.
        
        4. **Dropout**
        
        Случайно отключает долю нейронов в слое с вероятностью $p$:
        $$
        y = \begin{cases} 
        \frac{x}{1-p} & \text{с вероятностью } 1-p \\
        0 & \text{с вероятностью } p
        \end{cases}
        $$
        
        5. **Выходной слой (Output Layer)**
        
        Применяет сигмоидальную функцию активации для вывода вероятности:
        $$
        \sigma(z) = \frac{1}{1 + e^{-z}}
        $$
        
        6. **Функция потерь (Loss Function)**
        
        Используется бинарная кросс-энтропия для задач бинарной классификации:
        $$
        \text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
        $$
        
        7. **Оптимизатор Adam**
        
        Адаптивный метод моментума и RMSProp:
        $$
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
        $$
        $$
        v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
        $$
        Корректировка смещений:
        $$
        \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
        $$
        $$
        \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
        $$
        Обновление параметров:
        $$
        \theta_t = \theta_{t-1} - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
        $$
        
        8. **Метрика AUC**
        
        Оценка качества бинарной классификации путем измерения площади под кривой ROC:
        $$
        \text{AUC} = \int_0^1 \text{TPR}(FPR) \, d(\text{FPR})
        $$
        
        ## Процесс настройки гиперпараметров
        
        Используется `RandomSearch` для поиска лучших комбинаций гиперпараметров:
        - `units_input`: Количество нейронов в первом слое.
        - `dropout_input`: Вероятность Dropout в первом слое.
        - `num_layers`: Количество скрытых слоев.
        - `units`: Количество нейронов в каждом скрытом слое.
        - `dropout`: Вероятность Dropout в каждом скрытом слое.
        - `learning_rate`: Скорость обучения для оптимизатора.
    """)

    def build_model(hp):
        inputs = Input(shape=(X_train.shape[1],))
        x = Dense(units=hp.Int('units_input', min_value=64, max_value=512, step=32), activation='relu')(inputs)
        x = BatchNormalization()(x)
        x = Dropout(rate=hp.Float('dropout_input', min_value=0.0, max_value=0.5, step=0.1))(x)

        for i in range(hp.Int('num_layers', 1, 5)):
            x = Dense(units=hp.Int(f'units_{i}', min_value=64, max_value=512, step=32), activation='relu')(x)
            x = BatchNormalization()(x)
            x = Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1))(x)

        outputs = Dense(1, activation='sigmoid')(x)
        model = Model(inputs, outputs)
        model.compile(
            optimizer=Adam(learning_rate=hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='LOG')),
            loss='binary_crossentropy',
            metrics=[AUC(name='AUC')]
        )
        return model

    def get_tuner(X_train):
        tuner_dir = 'tuner_dir'
        project_name = 'complex_nn_model'

        if not os.path.exists(tuner_dir):
            os.makedirs(tuner_dir)

        tuner = RandomSearch(
            build_model,
            objective='val_AUC',
            max_trials=50,
            executions_per_trial=2,
            directory=tuner_dir,
            project_name=project_name
        )

        # Проверяем, существует ли уже сохранённый поиск
        if os.path.exists(os.path.join(tuner_dir, project_name, "oracle.json")):
            tuner.reload()

        return tuner

    def build_and_train_best_model(X_train, y_train, X_test, y_test, model_path, history_path):
        if os.path.exists(model_path):
            from keras.models import load_model
            best_model = load_model(model_path)
            with open(history_path, 'r') as f:
                history = json.load(f)
            st.write("Модель загружена из сохраненного файла.")

        else:
            tuner_dir = 'tuner_dir'
            project_name = 'complex_nn_model'

            if not os.path.exists(tuner_dir):
                os.makedirs(tuner_dir)

            tuner = RandomSearch(
                build_model,
                objective='val_AUC',
                max_trials=50,
                executions_per_trial=2,
                directory=tuner_dir,
                project_name=project_name
            )

            # Проверяем, существует ли уже сохранённый поиск
            if os.path.exists(os.path.join(tuner_dir, project_name, "oracle.json")):
                tuner.reload()

            tuner.search_space_summary()

            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])

            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

            st.subheader('Лучшие гиперпараметры:')
            st.write(f"""
            - units_input: {best_hps.get('units_input')}
            - dropout_input: {best_hps.get('dropout_input')}
            - num_layers: {best_hps.get('num_layers')}
            - units per layer: {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}
            - dropout per layer: {[best_hps.get(f'dropout_{i}') for i in range(best_hps.get('num_layers'))]}
            - learning_rate: {best_hps.get('lr')}
            """)

            best_model = tuner.hypermodel.build(best_hps)
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            history = best_model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=100,
                batch_size=32,
                callbacks=[early_stopping]
            )
            best_model.save(model_path)
            st.write("Модель обучена и сохранена.")
            history = history.history
            with open(history_path, 'w') as f:
                json.dump(history, f)
        return best_model, history

    @st.cache_data
    def evaluate_nn(true, pred, train=True):
        clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))
        if train:
            st.write("### Результат обучения")
        else:
            st.write("### Результат тестирования")

        st.write(f"Точность модели: {accuracy_score(true, pred) * 100:.2f}%")
        st.write(f"Отчет по классификации:", pd.DataFrame(clf_report).transpose())

    def plot_learning_evolution(history):
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))
        axes[0].plot(history['loss'], label='Loss')
        axes[0].plot(history['val_loss'], label='val_Loss')
        axes[0].set_title('Эволюция потерь во время обучения')
        axes[0].legend()
        axes[1].plot(history['AUC'], label='AUC')
        axes[1].plot(history['val_AUC'], label='val_AUC')
        axes[1].set_title('Эволюция AUC во время обучения')
        axes[1].legend()
        st.pyplot(fig)

    @st.cache_data
    def plot_confusion_matrix(y_true, y_pred, normalize=False):
        cm = confusion_matrix(y_true, y_pred)
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm = cm.round(3)

        fig = px.imshow(
            cm, labels=dict(x="Предсказанный класс", y="Истинный класс"), x=['Нет', 'Да'], y=['Нет', 'Да'],
            title='Нормализованная матрица ошибок' if normalize else 'Матрица ошибок',
            color_continuous_scale='Blues'
        )

        fig.update_xaxes(tickangle=45, tickmode='array', tickvals=np.arange(2), ticktext=['Нет', 'Да'])
        fig.update_yaxes(tickangle=45, tickmode='array', tickvals=np.arange(2), ticktext=['Нет', 'Да'])
        thresh = cm.max() / 2
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                fig.add_annotation(
                    x=i, y=j,
                    text=str(cm[j, i]),
                    showarrow=False,
                    font=dict(color="white" if cm[j, i] > thresh else "black"),
                    align="center"
                )
        return fig

    # Streamlit interface
    st.title('Моделирование нейронной сети с Keras Tuner')
    st.write('Настройка гиперпараметров модели с использованием Random Search')

    model_path = str(current_dir / "models" / 'best_model.keras')
    history_path = str(current_dir / "models" / 'history.json')
    best_model, history = build_and_train_best_model(X_train, y_train, X_test, y_test, model_path, history_path)

    plot_learning_evolution(history)

    # Предсказания на тренировочной и тестовой выборках
    y_train_pred = (best_model.predict(X_train) > 0.5).astype("int32")
    y_test_pred = (best_model.predict(X_test) > 0.5).astype("int32")

    # Оценка модели на тренировочных данных
    c1, c2 = st.columns(2)
    with c1:
        evaluate_nn(y_train, y_train_pred, train=True)
    with c2:
        st.plotly_chart(plot_confusion_matrix(y_train, y_train_pred))

    # Оценка модели на тестовых данных
    c1, c2 = st.columns(2)
    with c1:
        evaluate_nn(y_test, y_test_pred, train=False)
    with c2:
        st.plotly_chart(plot_confusion_matrix(y_test, y_test_pred))

    st.title("Ввод данных для предсказания кредитного риска")

    # Ввод данных клиента
    st.header("Данные клиента")
    col1, col2 = st.columns(2)

    with col1:
        age = st.number_input('Возраст', min_value=18, max_value=100, value=25, step=1)
        region = st.selectbox('Регион', options=list(encoders['REGION'].classes_))
        gender = st.selectbox('Пол', options=list(encoders['GENDER'].classes_))
        income = st.number_input('Доход', min_value=0, max_value=1000000, value=50000, step=1000)
        pdn = st.number_input('Показатель долговой нагрузки (ПДН)', min_value=0.0, max_value=100.0, value=10.0,
                              step=0.1)
        ip_flag = st.selectbox('Является ИП?', options=list(encoders['IP_FLAG'].classes_))
        sme_flag = st.selectbox('Является сотрудником организации?', options=list(encoders['SME_FLAG'].classes_))

    with col2:
        marital_status = option_menu(
            "Семейное положение",
            options=list(encoders['MARITAL_STATUS'].classes_),
            menu_icon="house-heart",
            icons=["person", "people", "person-x", "question-circle"],
            default_index=0
        )

        employee_flag = st.selectbox('Является сотрудником?', options=list(encoders['EMPLOYEE_FLAG'].classes_))
        refugee_flag = st.selectbox('Является беженцем?', options=list(encoders['REFUGEE_FLAG'].classes_))

    # Выбор ввода данных по кредиту или по карте
    st.header("Выбор данных")
    data_type = option_menu(
        "Выберите тип данных для ввода",
        options=["Кредит", "Карта"],
        menu_icon="list",
        icons=["credit-card", "wallet"],
        default_index=0
    )

    import datetime
    def parse_date(date):
        year = date.year
        month = date.month
        day = date.day
        dayofweek = date.weekday()
        return year, month, day, dayofweek

    if data_type == "Кредит":
        st.subheader("Данные по кредиту")
        col1, col2 = st.columns(2)

        with col1:
            credit_type = st.selectbox('Тип кредита', options=list(encoders['CREDIT_TYPE'].classes_))
            credit_purchase = st.selectbox('Цель кредита', options=list(encoders['CREDIT_PURCHASE'].classes_))
            product_code = st.text_input('Код кредитного продукта', 'None')
            term = st.number_input('Срок кредита (в месяцах)', min_value=0, max_value=360, value=12, step=1)

        with col2:
            curr_rate_nval = st.number_input('Процентная ставка', min_value=0.0, max_value=100.0, value=10.0, step=0.1)
            value_dt = st.date_input('Дата выдачи кредита', value=datetime.date.today())
            orig_amount = st.number_input('Сумма кредита', min_value=0, max_value=1000000, value=100000, step=1000)

        value_dt_year, value_dt_month, value_dt_day, value_dt_dayofweek = parse_date(value_dt)
        credit_data = {
            'CREDIT_TYPE'        : credit_type,
            'CREDIT_PURCHASE'    : credit_purchase,
            'PRODUCT_CODE'       : product_code,
            'TERM'               : term,
            'ORIG_AMOUNT'        : orig_amount,
            'CURR_RATE_NVAL'     : curr_rate_nval,
            'VALUE_DT_YEAR'      : value_dt_year,
            'VALUE_DT_MONTH'     : value_dt_month,
            'VALUE_DT_DAY'       : value_dt_day,
            'VALUE_DT_DAYOFWEEK' : value_dt_dayofweek,
        }

    elif data_type == "Карта":
        st.subheader("Данные по карте")
        col1, col2 = st.columns(2)

        with col1:
            card_type = st.selectbox('Тип карты', options=list(encoders['CARD_TYPE'].classes_))
            product_code = st.text_input('Код продукта', 'None')
            cc_limit_nval = st.number_input('Лимит по карте', min_value=0, max_value=1000000, value=50000, step=1000)

        with col2:
            cc_grace_period = st.number_input('Грейс-период', min_value=0, max_value=365, value=30, step=1)
            curr_rate = st.number_input('Процентная ставка по карте', min_value=0.0, max_value=100.0, value=10.0,
                                        step=0.1)
            open_dt = st.date_input('Дата открытия карты', value=datetime.date.today())

        open_dt_year, open_dt_month, open_dt_day, open_dt_dayofweek = parse_date(open_dt)
        card_data = {
            'CARD_TYPE'         : card_type,
            'PRODUCT_CODE'      : product_code,
            'CС_LIMIT_NVAL'     : cc_limit_nval,
            'СС_GRACE_PERIOD'   : cc_grace_period,
            'CURР_RATE'         : curr_rate,
            'OPEN_DT_YEAR'      : open_dt_year,
            'OPEN_DT_MONTH'     : open_dt_month,
            'OPEN_DT_DAY'       : open_dt_day,
            'OPEN_DT_DAYOFWEEK' : open_dt_dayofweek,
        }

    st.header("Выбор модели")
    model_choice = st.radio(
        "Выберите модель для прогноза",
        options=["Neural Network", "Logistic Regression"]
    )

    if st.button("Отправить", type="primary", use_container_width=True):
        client_data = {
            'AGE'            : age,
            'REGION'         : region,
            'GENDER'         : gender,
            'INCOME'         : income,
            'MARITAL_STATUS' : marital_status,
            'IP_FLAG'        : ip_flag,
            'SME_FLAG'       : sme_flag,
            'EMPLOYEE_FLAG'  : employee_flag,
            'REFUGEE_FLAG'   : refugee_flag,
            'PDN'            : pdn
        }

        if data_type == "Кредит":
            user_data = {**client_data, **credit_data}
        elif data_type == "Карта":
            user_data = {**client_data, **card_data}

        # Замена пустых значений на -1 или 'None'
        for key in user_data:
            if user_data[key] in [None, '']:
                if isinstance(user_data[key], str):
                    user_data[key] = 'None'
                else:
                    user_data[key] = -1

        user_df = pd.DataFrame(columns=X_test.columns)
        user_df = user_df._append(user_data, ignore_index=True).fillna({
            'CREDIT_TYPE'        : -1,
            'CREDIT_PURCHASE'    : 'None',
            'PRODUCT_CODE'       : 'None',
            'TERM'               : -1,
            'ORIG_AMOUNT'        : -1,
            'CURR_RATE_NVAL'     : -1,
            'VALUE_DT_YEAR'      : -1,
            'VALUE_DT_MONTH'     : -1,
            'VALUE_DT_DAY'       : -1,
            'VALUE_DT_DAYOFWEEK' : -1,
            'CARD_TYPE'          : 'None',
            'CС_LIMIT_NVAL'      : -1,
            'СС_GRACE_PERIOD'    : -1,
            'CURR_RATE'          : -1,
            'OPEN_DT_YEAR'       : -1,
            'OPEN_DT_MONTH'      : -1,
            'OPEN_DT_DAY'        : -1,
            'OPEN_DT_DAYOFWEEK'  : -1,
            'AGE'                : -1,
            'REGION'             : 'None',
            'GENDER'             : -1,
            'INCOME'             : -1,
            'MARITAL_STATUS'     : 'None',
            'IP_FLAG'            : -1,
            'SME_FLAG'           : -1,
            'EMPLOYEE_FLAG'      : -1,
            'REFUGEE_FLAG'       : -1,
            'PDN'                : -1
        })
        # Трансформирование данных с использованием кодировщиков
        for feature, le in encoders.items():
            try:
                user_df[feature] = le.transform(user_df[feature])
            except ValueError:
                user_df[feature] = user_df[feature].apply(lambda x: le.classes_[0] if x not in le.classes_ else x)
                user_df[feature] = le.transform(user_df[feature])
            except KeyError:
                continue
        # Прогнозирование
        user_df = scaler.transform(user_df)
        if model_choice == "Logistic Regression":
            pred_proba = log_reg.predict_proba(user_df)[:, 1][0]
        else:
            pred_proba = best_model.predict(user_df)[0][0]
        prediction = pred_proba * 100
        color = "green" if (pred_proba > 0.5).astype("int32") == 0 else "red"
        st.write(f"Прогноз успешно выполнен! Вероятность просрочки по кредиту: {prediction:.2f}")

        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=prediction,
            number={'suffix': "%"},
            gauge={'axis': {'range': [0, 100]}, 'bar': {'color': color}},
            title={"text": "Прогнозируемая вероятность просрочки"}
        ))

        fig.update_layout(paper_bgcolor="#f0f2f6", font={'color': color, 'family': "Arial"})
        st.plotly_chart(fig, use_container_width=True)

    st.title("Прогнозирование вероятности просрочки по кредиту на файле")
    st.markdown("""
        Загрузите файл Excel с данными о клиентах, кредитах и картах. 
        Файл должен содержать три страницы: `клиенты`, `кредиты` и `карты`.
    """)
    st.image(str(current_dir / "images" / "example.png"), use_column_width='auto')
    uploaded_file = st.file_uploader("Выберите файл Excel", type=["xlsx"])

    @st.cache_data
    def load_and_process_file(uploaded_file, is_test=False):
        client_ds = pd.read_excel(uploaded_file, sheet_name='клиенты').drop_duplicates()
        credit_ds = pd.read_excel(uploaded_file, sheet_name='кредиты').drop_duplicates()
        card_ds = pd.read_excel(uploaded_file, sheet_name='карты').drop_duplicates()

        credit_ds['ROW'] = credit_ds.index
        card_ds['ROW'] = card_ds.index

        # Объединение данных
        # df_credit = pd.merge(client_ds, credit_ds, on='CLIENT_ID', how='right')
        # df_card = pd.merge(client_ds, card_ds, on='CLIENT_ID', how='right')

        df_credit = pd.merge(client_ds, credit_ds, on='CLIENT_ID', how='right', suffixes=('_client', ''))
        df_card = pd.merge(client_ds, card_ds, on='CLIENT_ID', how='right', suffixes=('_client', ''))

        df_credit['PAGE'] = 'кредиты'
        df_card['PAGE'] = 'карты'

        df = pd.concat([df_credit, df_card]).copy(deep=True)

        if not is_test:
            df['TARGET'] = df['OVERDUE_IND'].fillna(df['СС_OVERДUE_IND'])

        df['TERM'] = df['TERM'].str.replace('M', '', regex=False)
        df['VALUE_DT'] = pd.to_datetime(df['VALUE_DT'], format='%d.%m.%Y', errors='coerce')
        df['OPEN_DT'] = pd.to_datetime(df['OPEN_DT'], format='%d.%м.%Y', errors='coerce')

        # Извлечение года, месяца, дня и дня недели
        df['OPEN_DT_YEAR'] = df['OPEN_DT'].dt.year
        df['OPEN_DT_MONTH'] = df['OPEN_DT'].dt.month
        df['OPEN_DT_DAY'] = df['OPEN_DT'].dt.day
        df['OPEN_DT_DAYOFWEEK'] = df['OPEN_DT'].dt.dayofweek

        df['VALUE_DT_YEAR'] = df['VALUE_DT'].dt.year
        df['VALUE_DT_MONTH'] = df['VALUE_DT'].dt.month
        df['VALUE_DT_DAY'] = df['VALUE_DT'].dt.day
        df['VALUE_DT_DAYOFWEEK'] = df['VALUE_DT'].dt.dayofweek

        # Заполнение пропущенных значений
        df.fillna({
            'CREDIT_TYPE'        : -1,
            'CREDIT_PURCHASE'    : 'None',
            'PRODUCT_CODE'       : 'None',
            'TERM'               : -1,
            'ORIG_AMOUNT'        : -1,
            'CURR_RATE_NVAL'     : -1,
            'VALUE_DT'           : '1900-01-01',
            'CARD_TYPE'          : 'None',
            'CС_LIMIT_NVAL'      : -1,
            'СС_GRACE_PERIOD'    : -1,
            'CURR_RATE'          : -1,
            'OPEN_DT'            : '1900-01-01',
            "OPEN_DT_YEAR"       : -1,
            "OPEN_DT_MONTH"      : -1,
            "OPEN_DT_DAY"        : -1,
            "OPEN_DT_DAYOFWEEK"  : -1,
            "VALUE_DT_YEAR"      : -1,
            "VALUE_DT_MONTH"     : -1,
            "VALUE_DT_DAY"       : -1,
            "VALUE_DT_DAYOFWEEK" : -1,
            'AGE'                : -1,
            'REGION'             : 'None',
            'GENDER'             : -1,
            'INCOME'             : -1,
            'MARITAL_STATUS'     : 'None',
            'IP_FLAG'            : -1,
            'SME_FLAG'           : -1,
            'EMPLOYEE_FLAG'      : -1,
            'REFUGEE_FLAG'       : -1,
            'PDN'                : -1
        }, inplace=True)
        df['TERM'] = df['TERM'].astype(int)
        df['AGE'] = df['AGE'].astype(int)

        # Удаление ненужных столбцов
        df = df.drop(['VALUE_DT', 'OPEN_DT', 'ORGANIZATION', 'JOB'], axis=1)
        if not is_test:
            df = df.drop(['OVERDUE_IND', 'СС_OVERДUE_IND'], axis=1)
        return df

    if uploaded_file is not None:
        test_df = load_and_process_file(uploaded_file, is_test=True)
        st.subheader("Загруженные данные")
        st.dataframe(test_df, use_container_width=True)

        # Кодирование категориальных признаков в тестовых данных
        categorical_features = ['REGION', 'GENDER', 'MARITAL_STATUS', 'IP_FLAG', 'SME_FLAG', 'EMPLOYEE_FLAG',
                                'REFUGEE_FLAG', 'CREDIT_TYPE', 'CREDIT_PURCHASE', 'PRODUCT_CODE', 'CARD_TYPE']

        encoders = {feature: LabelEncoder().fit(test_df[feature]) for feature in categorical_features}

        for feature, le in encoders.items():
            try:
                test_df[feature] = le.transform(test_df[feature])
            except ValueError as e:
                # Замена отсутствующих значений на le.classes_[0]
                test_df[feature] = test_df[feature].apply(
                    lambda x: le.classes_[0] if x not in le.classes_ else x
                )
                test_df[feature] = le.transform(test_df[feature])
            except KeyError as e:
                continue

        # Предполагаем, что масштабировщик обучен на обучающей выборке
        X_test_df = scaler.transform(test_df.drop(['ROW', 'PAGE', 'CLIENT_ID'], axis=1))

        # Выбор модели для прогноза
        model_choice = st.radio(
            "Выберите модель для прогноза",
            options=["Neural Network", "Logistic Regression"],
            key='select_for_file'
        )

        # Прогнозирование
        if model_choice == "Logistic Regression":
            predict = log_reg.predict(X_test_df)
        else:
            predict = (best_model.predict(X_test_df) > 0.5).astype("int32")

        # Вывод предсказаний с сохранением информации о строке и странице
        results = pd.DataFrame({
            'CLIENT_ID': test_df['CLIENT_ID'],
            'ROW': test_df['ROW'],
            'PAGE': test_df['PAGE'],
            'PREDICT': predict.flatten()
        })

        st.header("Результаты прогнозирования")
        credit_results = results[results['PAGE'] == 'кредиты']
        card_results = results[results['PAGE'] == 'карты']

        st.subheader("Результаты по кредитам")
        st.dataframe(credit_results, use_container_width=True)

        st.subheader("Результаты по картам")
        st.dataframe(card_results, use_container_width=True)

    #
    #
    # @st.cache_data
    # def load_and_process_file(uploaded_file, is_test=False):
    #     client_ds = pd.read_excel(uploaded_file, sheet_name='клиенты').drop_duplicates()
    #     credit_ds = pd.read_excel(uploaded_file, sheet_name='кредиты').drop_duplicates()
    #     card_ds = pd.read_excel(uploaded_file, sheet_name='карты').drop_duplicates()
    #
    #     # Объединение данных
    #     # Начнем с объединения client_ds с credit_ds и card_ds по CLIENT_ID
    #     df = pd.concat([
    #         pd.merge(client_ds, credit_ds, on='CLIENT_ID', how='right'),
    #         pd.merge(client_ds, card_ds, on='CLIENT_ID', how='right')
    #     ]).copy(deep=True)
    #
    #     if not is_test:
    #         df['TARGET'] = df['OVERDUE_IND'].fillna(df['СС_OVERDUE_IND'])
    #
    #     df['TERM']       = df['TERM'].str.replace('M', '', regex=False)
    #     df['VALUE_DT']   = pd.to_datetime(df['VALUE_DT'], format='%d.%m.%Y', errors='coerce')
    #     df['OPEN_DT']    = pd.to_datetime(df['OPEN_DT'], format='%d.%m.%Y', errors='coerce')
    #
    #     # Извлечение года, месяца, дня и дня недели
    #     df['OPEN_DT_YEAR']      = df['VALUE_DT'].dt.year
    #     df['OPEN_DT_MONTH']     = df['VALUE_DT'].dt.month
    #     df['OPEN_DT_DAY']       = df['VALUE_DT'].dt.day
    #     df['OPEN_DT_DAYOFWEEK'] = df['VALUE_DT'].dt.dayofweek
    #
    #
    #     # Извлечение года, месяца, дня и дня недели
    #     df['VALUE_DT_YEAR']      = df['VALUE_DT'].dt.year
    #     df['VALUE_DT_MONTH']     = df['VALUE_DT'].dt.month
    #     df['VALUE_DT_DAY']       = df['VALUE_DT'].dt.day
    #     df['VALUE_DT_DAYOFWEEK'] = df['VALUE_DT'].dt.dayofweek
    #
    #     # Заполнение пропущенных значений
    #     df.fillna({
    #         'CREDIT_TYPE'        : -1,
    #         'CREDIT_PURCHASE'    : 'None',
    #         'PRODUCT_CODE_x'     : 'None',
    #         'TERM'               : -1,
    #         'ORIG_AMOUNT'        : -1,
    #         'CURR_RATE_NVAL'     : -1,
    #         'VALUE_DT'           : '1900-01-01',
    #         'CARD_TYPE'          : 'None',
    #         'PRODUCT_CODE_y'     : 'None',
    #         'CС_LIMIT_NVAL'      : -1,
    #         'СС_GRACE_PERIOD'    : -1,
    #         'CURR_RATE'          : -1,
    #         'OPEN_DT'            : '1900-01-01',
    #         "OPEN_DT_YEAR"       : -1,
    #         "OPEN_DT_MONTH"      : -1,
    #         "OPEN_DT_DAY"        : -1,
    #         "OPEN_DT_DAYOFWEEK"  : -1,
    #         "VALUE_DT_YEAR"      : -1,
    #         "VALUE_DT_MONTH"     : -1,
    #         "VALUE_DT_DAY"       : -1,
    #         "VALUE_DT_DAYOFWEEK" : -1,
    #         'AGE'                : -1,
    #         'REGION'             : 'None',
    #         'GENDER'             : -1,
    #         'JOB'                : -1,
    #         'INCOME'             : -1,
    #         'MARITAL_STATUS'     : 'None',
    #         'IP_FLAG'            : -1,
    #         'SME_FLAG'           : -1,
    #         'EMPLOYEE_FLAG'      : -1,
    #         'REFUGEE_FLAG'       : -1,
    #         'PDN'                : -1
    #
    #     }, inplace=True)
    #
    #     df['TERM'] = df['TERM'].astype(int)
    #     df['AGE'] = df['AGE'].astype(int)
    #
    #     # Удаление ненужных столбцов
    #     df = df.drop(['VALUE_DT', 'OPEN_DT', 'CLIENT_ID', 'ORGANIZATION', 'JOB', ], axis=1)
    #     if not is_test: df = df.drop(['OVERDUE_IND', 'СС_OVERDUE_IND'], axis=1)
    #     return df
    #
    # if uploaded_file != None:
    #     test_df = load_and_process_file(uploaded_file, is_test=True)
    #     st.subheader("Загруженные данные")
    #     st.dataframe(test_df, use_container_width=True)
    #
    #     # Кодирование категориальных признаков в тестовых данных
    #     for feature, le in encoders.items():
    #         try:
    #             test_df[feature] = le.transform(test_df[feature])
    #         except ValueError as e:
    #             # Замена отсутствующих значений на le.classes_[0]
    #             test_df[feature] = test_df[feature].apply(
    #                 lambda x: le.classes_[0] if x not in le.classes_ else x
    #             )
    #             test_df[feature] = le.transform(test_df[feature])
    #         except KeyError as e:
    #             continue
    #
    #     X_test_df = scaler.transform(test_df)
    #
    #     # Выбор модели для прогноза
    #     model_choice = st.radio(
    #         "Выберите модель для прогноза",
    #         options=["Neural Network", "Logistic Regression"],
    #         key='select_for_file'
    #     )
    #     # Прогнозирование
    #     if model_choice == "Logistic Regression":
    #         predict = log_reg.predict(X_test_df)
    #     else:
    #         predict =  (best_model.predict(X_test_df) > 0.5).astype("int32")
    #     st.dataframe( pd.DataFrame(predict, columns=["Прогноз"]).T, use_container_width=True)
